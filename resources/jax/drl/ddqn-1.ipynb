{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=2)(x)\n",
    "        return x\n",
    "\n",
    "def initialize_model_and_optimizer(rng, learning_rate=1e-3):\n",
    "    model = QNetwork()\n",
    "    params = model.init(rng, jnp.zeros((1, 4)))[\"params\"]\n",
    "    tx = optax.adam(learning_rate)\n",
    "    opt_state = tx.init(params)\n",
    "    return model, params, tx, opt_state\n",
    "\n",
    "#@jax.jit\n",
    "def train_step(model, params, opt_state, target_params, batch, tx):\n",
    "    def loss_fn(params):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        q_values = model.apply({'params':params}, states)\n",
    "        q_values = jnp.take_along_axis(q_values, actions[:, None], axis=-1).squeeze()\n",
    "        \n",
    "        next_q_values_online = model.apply({'params':params}, next_states)\n",
    "        next_q_values_target = model.apply({'params':target_params}, next_states)\n",
    "        \n",
    "        next_actions = jnp.argmax(next_q_values_online, axis=-1) #\n",
    "        next_q_values = jnp.take_along_axis(next_q_values_target, next_actions[:, None], axis=-1).squeeze()\n",
    "\n",
    "        targets = rewards + (1 - dones) * 0.99 * next_q_values\n",
    "        loss = jnp.mean((targets - q_values) ** 2)\n",
    "        return loss\n",
    "\n",
    "    grad = jax.grad(loss_fn)(params)\n",
    "    updates, opt_state = tx.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, opt_state\n",
    "\n",
    "def epsilon_greedy_policy(model, params, observation, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    q_values = model.apply({'params':params}, jnp.expand_dims(observation, 0))\n",
    "    return int(jnp.argmax(q_values))\n",
    "\n",
    "def update_target_params(state_params, target_params, tau=0.001):\n",
    "    # there are two ways to update target q-network\n",
    "    # 1. sync target network and online network at a fixed frequency such as every 100 steps\n",
    "    # 2. soft update: update at each step but with small amount of updates controlled by tau\n",
    "    return jax.tree_map(lambda x, y: tau * x + (1 - tau) * y, state_params, target_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\").env\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "model, params, tx, opt_state = initialize_model_and_optimizer(rng)\n",
    "target_params = params\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "\n",
    "num_episodes = 300\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "steps_since_target_update = 0\n",
    "max_episode_steps = 200\n",
    "\n",
    "reward_history = []\n",
    "for episode in range(num_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step_in_episode = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, params, observation, epsilon)\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_in_episode += 1\n",
    "\n",
    "        replay_buffer.append((observation, action, reward, next_observation, float(done)))\n",
    "        observation = next_observation\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            indices = random.sample(range(len(replay_buffer)), batch_size)\n",
    "            batch = [jnp.stack([replay_buffer[i][j] for i in indices]) for j in range(5)]\n",
    "            params, opt_state = train_step(model, params, opt_state, target_params, batch, tx)\n",
    "\n",
    "            # soft update target params\n",
    "            target_params = update_target_params(params, target_params)\n",
    "            \n",
    "        if done:\n",
    "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}\")\n",
    "    reward_history.append(episode_reward)\n",
    "\n",
    "    # stop training if the steps in one episode is too long\n",
    "    if step_in_episode >= max_episode_steps:\n",
    "        print(f\"Agent reached max_episode_steps in episode {episode}.\")\n",
    "        break\n",
    "    # stop training if average reward reaches requirement\n",
    "    # Calculate the average reward over the last 'solved_window' episodes\n",
    "    if episode >= 100:\n",
    "        avg_reward = np.mean(reward_history[-100:])\n",
    "        print(f'Episode: {episode}, Average Reward: {avg_reward}')\n",
    "\n",
    "        if avg_reward >= 175:\n",
    "            print(f\"CartPole-v1 solved in {episode} episodes!\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training \n",
    "def plot_moving_average_reward(episode_rewards, window_size=100):\n",
    "    cumsum_rewards = np.cumsum(episode_rewards)\n",
    "    moving_avg_rewards = (cumsum_rewards[window_size:] - cumsum_rewards[:-window_size]) / window_size\n",
    "\n",
    "    plt.plot(moving_avg_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title('Moving Average Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "plot_moving_average_reward(reward_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
