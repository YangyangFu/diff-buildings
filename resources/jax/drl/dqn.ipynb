{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(64)\n",
    "        self.dense2 = nn.Dense(64)\n",
    "        self.dense3 = nn.Dense(2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "# Define the agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min, batch_size, buffer_size=100000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "\n",
    "        self.model = QNetwork()\n",
    "        self.params = self.model.init(jax.random.PRNGKey(42), jnp.zeros((state_dim,)))\n",
    "        self.optimizer = optax.adam(learning_rate)\n",
    "        self.opt_state = self.optimizer.init(self.params)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, apply_epsilon=True):\n",
    "        \"\"\"\n",
    "        explore_noise: True for training, False for testing\n",
    "        \"\"\"\n",
    "        if apply_epsilon and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        q_values = self.model.apply(self.params, state)\n",
    "        return int(jnp.argmax(q_values))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = list(zip(*batch))\n",
    "        states = jnp.stack(states)\n",
    "        actions = jnp.array(actions)\n",
    "        rewards = jnp.array(rewards)\n",
    "        next_states = jnp.stack(next_states)\n",
    "        dones = jnp.array(dones, dtype=jnp.float32)\n",
    "\n",
    "        def loss_fn(params):\n",
    "            q_values = self.model.apply(params, states)\n",
    "            q_next = self.model.apply(params, next_states)\n",
    "            q_target = rewards + (1.0 - dones) * self.gamma * jnp.max(q_next, axis=1)\n",
    "            q_pred = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "            return jnp.mean((q_pred - q_target) ** 2)\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(self.params)\n",
    "        updates, self.opt_state = self.optimizer.update(grads, self.opt_state)\n",
    "        self.params = optax.apply_updates(self.params, updates)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 15.0\n",
      "Episode 2, Total Reward: 26.0\n",
      "Episode 3, Total Reward: 11.0\n",
      "Episode 4, Total Reward: 19.0\n",
      "Episode 5, Total Reward: 13.0\n",
      "Episode 6, Total Reward: 42.0\n",
      "Episode 7, Total Reward: 13.0\n",
      "Episode 8, Total Reward: 30.0\n",
      "Episode 9, Total Reward: 24.0\n",
      "Episode 10, Total Reward: 11.0\n",
      "Episode 11, Total Reward: 13.0\n",
      "Episode 12, Total Reward: 18.0\n",
      "Episode 13, Total Reward: 10.0\n",
      "Episode 14, Total Reward: 37.0\n",
      "Episode 15, Total Reward: 26.0\n",
      "Episode 16, Total Reward: 26.0\n",
      "Episode 17, Total Reward: 19.0\n",
      "Episode 18, Total Reward: 20.0\n",
      "Episode 19, Total Reward: 16.0\n",
      "Episode 20, Total Reward: 16.0\n",
      "Episode 21, Total Reward: 47.0\n",
      "Episode 22, Total Reward: 13.0\n",
      "Episode 23, Total Reward: 44.0\n",
      "Episode 24, Total Reward: 35.0\n",
      "Episode 25, Total Reward: 27.0\n",
      "Episode 26, Total Reward: 40.0\n",
      "Episode 27, Total Reward: 52.0\n",
      "Episode 28, Total Reward: 20.0\n",
      "Episode 29, Total Reward: 17.0\n",
      "Episode 30, Total Reward: 16.0\n",
      "Episode 31, Total Reward: 46.0\n",
      "Episode 32, Total Reward: 51.0\n",
      "Episode 33, Total Reward: 112.0\n",
      "Episode 34, Total Reward: 51.0\n",
      "Episode 35, Total Reward: 42.0\n",
      "Episode 36, Total Reward: 14.0\n",
      "Episode 37, Total Reward: 67.0\n",
      "Episode 38, Total Reward: 43.0\n",
      "Episode 39, Total Reward: 150.0\n",
      "Episode 40, Total Reward: 140.0\n",
      "Episode 41, Total Reward: 55.0\n",
      "Episode 42, Total Reward: 72.0\n",
      "Episode 43, Total Reward: 109.0\n",
      "Episode 44, Total Reward: 117.0\n",
      "Episode 45, Total Reward: 53.0\n",
      "Episode 46, Total Reward: 54.0\n",
      "Episode 47, Total Reward: 77.0\n",
      "Episode 48, Total Reward: 184.0\n",
      "Agent reached max_episode_steps in episode 48.\n",
      "Episode 49, Total Reward: 200.0\n",
      "Episode 50, Total Reward: 196.0\n",
      "Episode 51, Total Reward: 82.0\n",
      "Episode 52, Total Reward: 95.0\n",
      "Episode 53, Total Reward: 98.0\n",
      "Agent reached max_episode_steps in episode 53.\n",
      "Episode 54, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 54.\n",
      "Episode 55, Total Reward: 200.0\n",
      "Episode 56, Total Reward: 103.0\n",
      "Agent reached max_episode_steps in episode 56.\n",
      "Episode 57, Total Reward: 200.0\n",
      "Episode 58, Total Reward: 151.0\n",
      "Agent reached max_episode_steps in episode 58.\n",
      "Episode 59, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 59.\n",
      "Episode 60, Total Reward: 200.0\n",
      "Episode 61, Total Reward: 180.0\n",
      "Agent reached max_episode_steps in episode 61.\n",
      "Episode 62, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 62.\n",
      "Episode 63, Total Reward: 200.0\n",
      "Episode 64, Total Reward: 146.0\n",
      "Episode 65, Total Reward: 103.0\n",
      "Agent reached max_episode_steps in episode 65.\n",
      "Episode 66, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 66.\n",
      "Episode 67, Total Reward: 200.0\n",
      "Episode 68, Total Reward: 146.0\n",
      "Episode 69, Total Reward: 97.0\n",
      "Episode 70, Total Reward: 127.0\n",
      "Episode 71, Total Reward: 195.0\n",
      "Episode 72, Total Reward: 119.0\n",
      "Agent reached max_episode_steps in episode 72.\n",
      "Episode 73, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 73.\n",
      "Episode 74, Total Reward: 200.0\n",
      "Episode 75, Total Reward: 188.0\n",
      "Agent reached max_episode_steps in episode 75.\n",
      "Episode 76, Total Reward: 200.0\n",
      "Episode 77, Total Reward: 139.0\n",
      "Agent reached max_episode_steps in episode 77.\n",
      "Episode 78, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 78.\n",
      "Episode 79, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 79.\n",
      "Episode 80, Total Reward: 200.0\n",
      "Episode 81, Total Reward: 171.0\n",
      "Agent reached max_episode_steps in episode 81.\n",
      "Episode 82, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 82.\n",
      "Episode 83, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 83.\n",
      "Episode 84, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 84.\n",
      "Episode 85, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 85.\n",
      "Episode 86, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 86.\n",
      "Episode 87, Total Reward: 200.0\n",
      "Episode 88, Total Reward: 195.0\n",
      "Agent reached max_episode_steps in episode 88.\n",
      "Episode 89, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 89.\n",
      "Episode 90, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 90.\n",
      "Episode 91, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 91.\n",
      "Episode 92, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 92.\n",
      "Episode 93, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 93.\n",
      "Episode 94, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 94.\n",
      "Episode 95, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 95.\n",
      "Episode 96, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 96.\n",
      "Episode 97, Total Reward: 200.0\n",
      "Episode 98, Total Reward: 167.0\n",
      "Episode 99, Total Reward: 153.0\n",
      "Agent reached max_episode_steps in episode 99.\n",
      "Episode 100, Total Reward: 200.0\n",
      "Agent reached max_episode_steps in episode 100.\n",
      "Episode 101, Total Reward: 200.0\n",
      "Episode: 100, Average Reward: 115.89\n",
      "Agent reached max_episode_steps in episode 101.\n",
      "Episode 102, Total Reward: 200.0\n",
      "Episode: 101, Average Reward: 117.63\n",
      "Agent reached max_episode_steps in episode 102.\n",
      "Episode 103, Total Reward: 200.0\n",
      "Episode: 102, Average Reward: 119.52\n",
      "Agent reached max_episode_steps in episode 103.\n",
      "Episode 104, Total Reward: 200.0\n",
      "Episode: 103, Average Reward: 121.33\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64)\n",
    "\n",
    "n_episodes = 500\n",
    "reward_history = []\n",
    "max_episode_steps=200 # env.spec.max_episode_steps\n",
    "reward_threshold=175 # env.spec.reward_threshold\n",
    "solved_window = 100\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_in_episode = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_in_episode += 1\n",
    "\n",
    "        # check if the max_episode_steps are met. if so, terminate this episode\n",
    "        if step_in_episode >= max_episode_steps:\n",
    "            print(f\"Agent reached max_episode_steps in episode {episode}.\")\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # stop training if average reward reaches requirement\n",
    "    # Calculate the average reward over the last 'solved_window' episodes\n",
    "    if episode >= solved_window:\n",
    "        avg_reward = np.mean(reward_history[-solved_window:])\n",
    "        print(f'Episode: {episode}, Average Reward: {avg_reward}')\n",
    "\n",
    "        if avg_reward >= reward_threshold:\n",
    "            print(f\"CartPole-v1 solved in {episode} episodes!\")\n",
    "            break\n",
    "\n",
    "# Plot the historical rewards\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Historical Rewards for CartPole-v1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a virtual display for rendering in docker\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Test the trained agent\n",
    "n_test_episodes = 10\n",
    "\n",
    "print(\"\\nTesting the trained agent...\")\n",
    "for episode in range(n_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    pre_screen = env.render()\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "        screen = env.render()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        plt.imshow(screen)\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
