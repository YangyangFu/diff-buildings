{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges for directly using SSM in the DRL is:\n",
    "- the unobservable state is not known when using the SSM for planning. For example, how to know the interior/exterior wall temperature for the RC model if it is used for planning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2., dtype=float32), Array(4., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if interpolaiton is differentiable in jax.numpy\n",
    "t = np.arange(10)\n",
    "y = t**2 + 3*t\n",
    "\n",
    "interp = jnp.interp(9.5, t, y) \n",
    "\n",
    "f = lambda x: jnp.interp(x, t, y)\n",
    "grad_fcn = jax.value_and_grad(f)\n",
    "grad_fcn(0.)\n",
    "grad_fcn(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[822.5739 ],\n",
       "       [-39.67265]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class SimpleDense(nn.Module):\n",
    "  features: int\n",
    "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "  bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    kernel = self.param('kernel',\n",
    "                        self.kernel_init, # Initialization function\n",
    "                        (inputs.shape[-1], self.features))  # shape info.\n",
    "    y = lax.dot_general(inputs, kernel,\n",
    "                        (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
    "    bias = self.param('bias', self.bias_init, (self.features,))\n",
    "    y = y + bias\n",
    "    return y\n",
    "\"\"\"\n",
    "class LSSM(nn.Module):\n",
    "    state_dim: int \n",
    "    action_dim: int\n",
    "    disturbancce_dim: int\n",
    "    output_dim: int\n",
    "    dt: float \n",
    "\n",
    "    initializer: Callable = nn.initializers.lecun_normal()\n",
    "    @nn.compact\n",
    "    def __call__(self, x, u, d):\n",
    "      A = self.param('A',\n",
    "                      self.initializer, # initialization\n",
    "                      (self.state_dim, self.state_dim)) # shape\n",
    "      Bu = self.param('Bu',\n",
    "                      self.initializer, # initialization\n",
    "                      (self.state_dim, self.action_dim))\n",
    "      Bd = self.param('Bd',\n",
    "                      self.initializer,\n",
    "                      (self.state_dim, self.disturbancce_dim))\n",
    "\n",
    "      C = self.param('C',\n",
    "                      self.initializer,\n",
    "                      (self.output_dim, self.state_dim))\n",
    "      D = self.param(\"D\",\n",
    "                      self.initializer,\n",
    "                      (self.output_dim, self.action_dim))\n",
    "      xdot = jnp.dot(A, x) + jnp.dot(Bu, u) + jnp.dot(Bd, d)\n",
    "\n",
    "      x_next = xdot * self.dt + x\n",
    "      y_next = jnp.dot(C, x_next) + jnp.dot(D, u)\n",
    "      \n",
    "      return xdot, x_next, y_next\n",
    "\n",
    "lssm = LSSM(state_dim=3, action_dim=1, disturbancce_dim=4, output_dim=2, dt=900.)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.PRNGKey(0), 2)\n",
    "x = jax.random.uniform(key1, (3,1))\n",
    "u = 0.\n",
    "d = jax.random.uniform(key1, (4,1))\n",
    "params = lssm.init(key2, x, u, d)\n",
    "xdot, x_next, y_next = lssm.apply(params, x, u, d)\n",
    "xdot\n",
    "x_next\n",
    "y_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 1.0646121e-03],\n",
       "        [-6.9011992e-05],\n",
       "        [-7.5943058e-07]], dtype=float32),\n",
       " Array([[19.95815 ],\n",
       "        [35.93789 ],\n",
       "        [24.999317]], dtype=float32),\n",
       " Array([[19.95815],\n",
       "        [ 0.     ]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# say we have an extact model of the system\n",
    "A = np.array([[-1.97402551e-04,  0.00000000e+00,  1.92610415e-04],\n",
    "              [0.00000000e+00, -3.30784418e-06,  2.00281620e-06],\n",
    "              [1.51360733e-06,  7.56564853e-07, -2.27017218e-06]])\n",
    "Bu = np.array([[9.63095932e-05], [0.], [0.]])\n",
    "Bd = np.array([[4.79213586e-06, 9.63095932e-05, 0.00000000e+00, 0.00000000e+00],\n",
    "               [1.30502798e-06, 0.00000000e+00, 2.00353963e-06, 0.00000000e+00],\n",
    "               [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.56838127e-07]])\n",
    "C = np.array([[1., 0., 0.],\n",
    "              [0., 0., 0.]])\n",
    "D = np.array([[0.],\n",
    "              [1.]])\n",
    "\n",
    "ABCD = {'A': A, 'Bu': Bu, 'Bd': Bd, 'C': C, 'D': D}\n",
    "exact_model_params = {\"params\": ABCD}\n",
    "\n",
    "# define the system\n",
    "x0 = np.array([19,36,25]).reshape(-1,1)\n",
    "u = 0\n",
    "d = np.array([0,0,0,0]).reshape(-1,1)\n",
    "dt = 900\n",
    "\n",
    "# simulate the system\n",
    "lssm = LSSM(state_dim=3, action_dim=1, disturbancce_dim=4, output_dim=2, dt=dt)\n",
    "lssm.apply(exact_model_params, x0, u, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(2)(x)\n",
    "        return x\n",
    "\n",
    "# we will not learn an environment model here instead we provide an ideal model\n",
    "#class EnvModel(nn.Module):\n",
    "#    @nn.compact\n",
    "#    def __call__(self, x):\n",
    "#        x = nn.Dense(256)(x)\n",
    "#        x = nn.relu(x)\n",
    "#        x = nn.Dense(256)(x)\n",
    "#        x = nn.relu(x)\n",
    "#        x = nn.Dense(256)(x)\n",
    "#        x = nn.relu(x)\n",
    "#        x = nn.Dense(3)(x)  # 3 outputs: 2 states [Tz for next step and power for next step] (although we have simple relationship between power and control action), and reward\n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import env\n",
    "# RC model parameters\n",
    "rc_params = [6.9789902e+03, 2.1591113e+04, 1.8807944e+05, 3.4490612e+00, 4.9556872e-01, 9.8289281e-02, 4.6257420e+00]\n",
    "x0 = np.array([20, 35.8, 26.])\n",
    "x_high = np.array([40., 80., 40.])\n",
    "x_low = np.array([10., 10., 10.])\n",
    "n_actions = 101\n",
    "u_high = [0]\n",
    "u_low = [-10.0] # -12\n",
    "\n",
    "# load disturbances\n",
    "file_path = os.path.abspath('')\n",
    "parent_path = os.path.dirname(file_path)\n",
    "data_path = os.path.join(parent_path, 'data/disturbance_1min.csv')\n",
    "data = pd.read_csv(data_path, index_col=[0])\n",
    "# assign time index\n",
    "t_base = 181*24*3600 # 7/1\n",
    "n = len(data)\n",
    "index = range(t_base, t_base + n*60, 60)\n",
    "data.index = index\n",
    "\n",
    "# sample\n",
    "dt = 900\n",
    "data = data.groupby([data.index // dt]).mean()\n",
    "index_dt = range(t_base, t_base + len(data)*dt, dt)\n",
    "data.index = index_dt \n",
    "\n",
    "# get disturbances for lssm\n",
    "t_d = index_dt\n",
    "disturbance_names = ['out_temp', 'qint_lump', 'qwin_lump', 'qradin_lump']\n",
    "disturbance = data[disturbance_names].values\n",
    "\n",
    "# RC Gym envionment\n",
    "ts = 195*24*3600\n",
    "ndays = 7\n",
    "te = ndays*24*3600 + ts\n",
    "weights = [100., 1., 0.] # for energy cost, dT, du\n",
    "\n",
    "env = gym.make(\"R4C3Discrete-v0\",\n",
    "            rc_params = rc_params,\n",
    "            x0 = x0,\n",
    "            x_high = x_high,\n",
    "            x_low = x_low,\n",
    "            n_actions = n_actions,\n",
    "            u_high = u_high,\n",
    "            u_low = u_low,\n",
    "            disturbances = (t_d, disturbance),\n",
    "            ts = ts,\n",
    "            te = te,\n",
    "            dt = dt,\n",
    "            weights = weights).env\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env is reset!\n",
      "(64, 27)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 2 required positional arguments: 'u' and 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mprint\u001b[39m(state_action_batch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m \u001b[39m# to model environment inputs\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m _, _, y_predictions \u001b[39m=\u001b[39m env_model\u001b[39m.\u001b[39;49mapply(env_model_params, state_action_batch)\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(y_predictions\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(ssss)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/flax/linen/module.py:842\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    841\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 842\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 2 required positional arguments: 'u' and 'd'"
     ]
    }
   ],
   "source": [
    "random.seed(41)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "episodes = 500\n",
    "batch_size = 64\n",
    "planning_steps = 5\n",
    "\n",
    "q_network = QNetwork()\n",
    "env_model = LSSM(state_dim=3, action_dim=1, disturbancce_dim=4, output_dim=2, dt=dt)\n",
    "\n",
    "params = q_network.init(jax.random.PRNGKey(0), jnp.zeros((state_dim,)))\n",
    "env_model_params = exact_model_params\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "env_model_optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "#env_model_opt_state = env_model_optimizer.init(env_model_params)\n",
    "\n",
    "@jax.jit\n",
    "def q_learning_update(params, opt_state, state, action, reward, next_state, done):\n",
    "    def loss_fn(params):\n",
    "        q_values = q_network.apply(params, state)\n",
    "        next_q_values = q_network.apply(params, next_state)\n",
    "        target = reward + gamma * jnp.max(next_q_values, axis=1) * (1 - done)\n",
    "        loss = jnp.mean((q_values[jnp.arange(q_values.shape[0]), action] - target) ** 2)\n",
    "        return loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "# env model update \n",
    "#@jax.jit\n",
    "def env_model_update(env_model_params, env_model_opt_state, state, action, next_state, reward):\n",
    "\n",
    "    def env_model_loss_fn(env_model_params):\n",
    "\n",
    "        state_action = jnp.hstack([state, action.reshape(-1,1)])#, axis=1\n",
    "        # predictions of Tz and Power\n",
    "        predictions = env_model.apply(env_model_params, state_action)\n",
    "        # target is next Tz and power\n",
    "        Tz_target = next_state[:,1]\n",
    "        power_target = next_state[:,4]\n",
    "        \n",
    "        target = jnp.stack([Tz_target, power_target, reward], axis=1)\n",
    "        print(target.shape, predictions.shape)\n",
    "        print(\"in model update\")\n",
    "        env_model_loss = jnp.mean(jnp.square(predictions - target))\n",
    "        return env_model_loss\n",
    "\n",
    "    env_model_loss, env_model_grads = jax.value_and_grad(env_model_loss_fn)(env_model_params)\n",
    "    env_model_updates, env_model_opt_state = env_model_optimizer.update(env_model_grads, env_model_opt_state)\n",
    "    env_model_params = optax.apply_updates(env_model_params, env_model_updates)\n",
    "    return env_model_params, env_model_opt_state\n",
    "\n",
    "memory = []\n",
    "reward_history = []\n",
    "reward_threshold=175 # env.spec.reward_threshold\n",
    "solved_window = 100\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset(seed=1)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_in_episode = 0\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = q_network.apply(params, jnp.expand_dims(jnp.array(state), axis=0))\n",
    "            action = jnp.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_in_episode += 1\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "\n",
    "            state_batch = jnp.array([s for (s, _, _, _, _) in batch])\n",
    "            action_batch = jnp.array([a for (_, a, _, _, _) in batch])\n",
    "            reward_batch = jnp.array([r for (_, _, r, _, _) in batch])\n",
    "            next_state_batch = jnp.array([ns for (_, _, _, ns, _) in batch])\n",
    "            done_batch = jnp.array([d for (_, _, _, _, d) in batch], dtype=jnp.float32)\n",
    "\n",
    "            params, opt_state = q_learning_update(params, opt_state, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "            # model learning: since we're using perfect env model, this is not needed.\n",
    "            #env_model_params, env_model_opt_state = env_model_update(env_model_params, env_model_opt_state, state_batch, action_batch, next_state_batch, reward_batch)\n",
    "\n",
    "            for _ in range(planning_steps):\n",
    "                planning_batch = random.sample(memory, batch_size)\n",
    "\n",
    "                state_batch = jnp.array([s for (s, _, _, _, _) in planning_batch])\n",
    "                action_batch = jnp.array([a for (_, a, _, _, _) in batch])\n",
    "\n",
    "                # (batch_size, )\n",
    "                state_action_batch = jnp.concatenate([state_batch, action_batch[:, np.newaxis]], axis=1)\n",
    "                _, _, y_predictions = env_model.apply(env_model_params, state_action_batch)\n",
    "                \n",
    "                # predictions are SSM states\n",
    "                Tz_next = y_predictions[:,0]\n",
    "                power_next = y_predictions[:,1]\n",
    "                next_state_batch = jnp.concatenate([Tz_next[:, np.newaxis], next_state_batch[:, 1:3], power_next[:, np.newaxis], next_state_batch[:, 4:]], axis=1)\n",
    "                reward_batch = power_next\n",
    "                done_batch = jnp.full(reward_batch.shape, False, dtype=jnp.float32)\n",
    "                #done_batch = (jnp.abs(jnp.sum(next_state_batch - state_batch, axis=1)) > 0.5).astype(jnp.float32)\n",
    "\n",
    "                params, opt_state = q_learning_update(params, opt_state, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "        # episode stopping: NOT IMPLEMENTED.\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # outputs\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    # stop training if average reward reaches requirement\n",
    "    # Calculate the average reward over the last 'solved_window' episodes\n",
    "    if episode >= solved_window:\n",
    "        avg_reward = np.mean(reward_history[-solved_window:])\n",
    "        print(f'Episode: {episode}, Average Reward: {avg_reward}')\n",
    "\n",
    "        if avg_reward >= reward_threshold:\n",
    "            print(f\"R4C3Discrete-v0 solved in {episode} episodes!\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the historical rewards\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Historical Rewards for CartPole-v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training \n",
    "def plot_moving_average_reward(episode_rewards, window_size=100):\n",
    "    cumsum_rewards = np.cumsum(episode_rewards)\n",
    "    moving_avg_rewards = (cumsum_rewards[window_size:] - cumsum_rewards[:-window_size]) / window_size\n",
    "\n",
    "    plt.plot(moving_avg_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title('Moving Average Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "plot_moving_average_reward(reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a virtual display for rendering in docker\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Test the trained agent\n",
    "\n",
    "print(\"\\nTesting the trained agent...\")\n",
    "env = gym.make(\"CartPole-v1\",render_mode='rgb_array').env\n",
    "state, _ = env.reset()\n",
    "state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "pre_screen = env.render()\n",
    "step_in_episode = 0\n",
    "\n",
    "while not done:\n",
    "    q_values = q_network.apply(params, jnp.expand_dims(jnp.array(state), axis=0))\n",
    "    action = jnp.argmax(q_values).item()\n",
    "    #action = agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "    screen = env.render()\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    step_in_episode += 1\n",
    "\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    # check if the max_episode_steps are met. if so, terminate this episode\n",
    "    if step_in_episode >= max_episode_steps:\n",
    "        print(f\"Agent reached max_episode_steps in test.\")\n",
    "        break\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    \n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pre_screen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
