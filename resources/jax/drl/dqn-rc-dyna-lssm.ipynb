{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2., dtype=float32), Array(4., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if interpolaiton is differentiable in jax.numpy\n",
    "t = np.arange(10)\n",
    "y = t**2 + 3*t\n",
    "\n",
    "interp = jnp.interp(9.5, t, y) \n",
    "\n",
    "f = lambda x: jnp.interp(x, t, y)\n",
    "grad_fcn = jax.value_and_grad(f)\n",
    "grad_fcn(0.)\n",
    "grad_fcn(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(2)(x)\n",
    "        return x\n",
    "\n",
    "class EnvModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(3)(x)  # 3 outputs: 2 states [Tz for next step and power for next step] (although we have simple relationship between power and control action), and reward\n",
    "        return x\n",
    "\n",
    "class LSSM(nn.Module):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "# RC model parameters\n",
    "rc_params = [6.9789902e+03, 2.1591113e+04, 1.8807944e+05, 3.4490612e+00, 4.9556872e-01, 9.8289281e-02, 4.6257420e+00]\n",
    "x0 = np.array([20, 35.8, 26.])\n",
    "x_high = np.array([40., 80., 40.])\n",
    "x_low = np.array([10., 10., 10.])\n",
    "n_actions = 101\n",
    "u_high = [0]\n",
    "u_low = [-10.0] # -12\n",
    "\n",
    "# load disturbances\n",
    "file_path = os.path.abspath('')\n",
    "parent_path = os.path.dirname(file_path)\n",
    "data_path = os.path.join(parent_path, 'data/disturbance_1min.csv')\n",
    "data = pd.read_csv(data_path, index_col=[0])\n",
    "# assign time index\n",
    "t_base = 181*24*3600 # 7/1\n",
    "n = len(data)\n",
    "index = range(t_base, t_base + n*60, 60)\n",
    "data.index = index\n",
    "\n",
    "# sample\n",
    "dt = 900\n",
    "data = data.groupby([data.index // dt]).mean()\n",
    "index_dt = range(t_base, t_base + len(data)*dt, dt)\n",
    "data.index = index_dt \n",
    "\n",
    "# get disturbances for lssm\n",
    "t_d = index_dt\n",
    "disturbance_names = ['out_temp', 'qint_lump', 'qwin_lump', 'qradin_lump']\n",
    "disturbance = data[disturbance_names].values\n",
    "\n",
    "# RC Gym envionment\n",
    "ts = 195*24*3600\n",
    "ndays = 7\n",
    "te = ndays*24*3600 + ts\n",
    "weights = [100., 1., 0.] # for energy cost, dT, du\n",
    "\n",
    "env = gym.make(\"R4C3Discrete-v0\",\n",
    "            rc_params = rc_params,\n",
    "            x0 = x0,\n",
    "            x_high = x_high,\n",
    "            x_low = x_low,\n",
    "            n_actions = n_actions,\n",
    "            u_high = u_high,\n",
    "            u_low = u_low,\n",
    "            disturbances = (t_d, disturbance),\n",
    "            ts = ts,\n",
    "            te = te,\n",
    "            dt = dt,\n",
    "            weights = weights).env\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env is reset!\n",
      "(64, 3) (64, 3)\n",
      "in model update\n",
      "Episode 1, Total Reward: -14.245113476420832\n",
      "env is reset!\n"
     ]
    }
   ],
   "source": [
    "random.seed(41)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "episodes = 500\n",
    "batch_size = 64\n",
    "planning_steps = 5\n",
    "\n",
    "q_network = QNetwork()\n",
    "env_model = EnvModel()\n",
    "\n",
    "params = q_network.init(jax.random.PRNGKey(0), jnp.zeros((state_dim,)))\n",
    "env_model_params = env_model.init(jax.random.PRNGKey(1), jnp.zeros((state_dim + 1,)))\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "env_model_optimizer = optax.adam(learning_rate)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "env_model_opt_state = env_model_optimizer.init(env_model_params)\n",
    "\n",
    "@jax.jit\n",
    "def q_learning_update(params, opt_state, state, action, reward, next_state, done):\n",
    "    def loss_fn(params):\n",
    "        q_values = q_network.apply(params, state)\n",
    "        next_q_values = q_network.apply(params, next_state)\n",
    "        target = reward + gamma * jnp.max(next_q_values, axis=1) * (1 - done)\n",
    "        loss = jnp.mean((q_values[jnp.arange(q_values.shape[0]), action] - target) ** 2)\n",
    "        return loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "# env model update \n",
    "@jax.jit\n",
    "def env_model_update(env_model_params, env_model_opt_state, state, action, next_state, reward):\n",
    "\n",
    "    def env_model_loss_fn(env_model_params):\n",
    "\n",
    "        state_action = jnp.hstack([state, action.reshape(-1,1)])#, axis=1\n",
    "        # predictions of Tz and Power\n",
    "        predictions = env_model.apply(env_model_params, state_action)\n",
    "        # target is next Tz and power\n",
    "        Tz_target = next_state[:,1]\n",
    "        power_target = next_state[:,4]\n",
    "        \n",
    "        target = jnp.stack([Tz_target, power_target, reward], axis=1)\n",
    "        print(target.shape, predictions.shape)\n",
    "        print(\"in model update\")\n",
    "        env_model_loss = jnp.mean(jnp.square(predictions - target))\n",
    "        return env_model_loss\n",
    "\n",
    "    env_model_loss, env_model_grads = jax.value_and_grad(env_model_loss_fn)(env_model_params)\n",
    "    env_model_updates, env_model_opt_state = env_model_optimizer.update(env_model_grads, env_model_opt_state)\n",
    "    env_model_params = optax.apply_updates(env_model_params, env_model_updates)\n",
    "    return env_model_params, env_model_opt_state\n",
    "\n",
    "memory = []\n",
    "reward_history = []\n",
    "reward_threshold=175 # env.spec.reward_threshold\n",
    "solved_window = 100\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset(seed=1)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_in_episode = 0\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = q_network.apply(params, jnp.expand_dims(jnp.array(state), axis=0))\n",
    "            action = jnp.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_in_episode += 1\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "\n",
    "            state_batch = jnp.array([s for (s, _, _, _, _) in batch])\n",
    "            action_batch = jnp.array([a for (_, a, _, _, _) in batch])\n",
    "            reward_batch = jnp.array([r for (_, _, r, _, _) in batch])\n",
    "            next_state_batch = jnp.array([ns for (_, _, _, ns, _) in batch])\n",
    "            done_batch = jnp.array([d for (_, _, _, _, d) in batch], dtype=jnp.float32)\n",
    "\n",
    "            params, opt_state = q_learning_update(params, opt_state, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "            env_model_params, env_model_opt_state = env_model_update(env_model_params, env_model_opt_state, state_batch, action_batch, next_state_batch, reward_batch)\n",
    "\n",
    "            for _ in range(planning_steps):\n",
    "                planning_batch = random.sample(memory, batch_size)\n",
    "\n",
    "                state_batch = jnp.array([s for (s, _, _, _, _) in planning_batch])\n",
    "                action_batch = jnp.array([a for (_, a, _, _, _) in batch])\n",
    "\n",
    "                state_action_batch = jnp.concatenate([state_batch, action_batch[:, np.newaxis]], axis=1)\n",
    "                predictions = env_model.apply(env_model_params, state_action_batch)\n",
    "\n",
    "                # replace Tz and power with predicted values for the next state\n",
    "                Tz_next = predictions[:, 0]\n",
    "                power_next = predictions[:, 1]\n",
    "                next_state_batch = jnp.concatenate([Tz_next[:, np.newaxis], next_state_batch[:, 1:3], power_next[:, np.newaxis], next_state_batch[:, 4:]], axis=1)\n",
    "                reward_batch = predictions[:, 2]\n",
    "                done_batch = jnp.full(reward_batch.shape, False, dtype=jnp.float32)\n",
    "                #done_batch = (jnp.abs(jnp.sum(next_state_batch - state_batch, axis=1)) > 0.5).astype(jnp.float32)\n",
    "\n",
    "                params, opt_state = q_learning_update(params, opt_state, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "        # episode stopping: NOT IMPLEMENTED.\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # outputs\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    # stop training if average reward reaches requirement\n",
    "    # Calculate the average reward over the last 'solved_window' episodes\n",
    "    if episode >= solved_window:\n",
    "        avg_reward = np.mean(reward_history[-solved_window:])\n",
    "        print(f'Episode: {episode}, Average Reward: {avg_reward}')\n",
    "\n",
    "        if avg_reward >= reward_threshold:\n",
    "            print(f\"R4C3Discrete-v0 solved in {episode} episodes!\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the historical rewards\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Historical Rewards for CartPole-v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training \n",
    "def plot_moving_average_reward(episode_rewards, window_size=100):\n",
    "    cumsum_rewards = np.cumsum(episode_rewards)\n",
    "    moving_avg_rewards = (cumsum_rewards[window_size:] - cumsum_rewards[:-window_size]) / window_size\n",
    "\n",
    "    plt.plot(moving_avg_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title('Moving Average Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "plot_moving_average_reward(reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a virtual display for rendering in docker\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Test the trained agent\n",
    "\n",
    "print(\"\\nTesting the trained agent...\")\n",
    "env = gym.make(\"CartPole-v1\",render_mode='rgb_array').env\n",
    "state, _ = env.reset()\n",
    "state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "pre_screen = env.render()\n",
    "step_in_episode = 0\n",
    "\n",
    "while not done:\n",
    "    q_values = q_network.apply(params, jnp.expand_dims(jnp.array(state), axis=0))\n",
    "    action = jnp.argmax(q_values).item()\n",
    "    #action = agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "    screen = env.render()\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    step_in_episode += 1\n",
    "\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    # check if the max_episode_steps are met. if so, terminate this episode\n",
    "    if step_in_episode >= max_episode_steps:\n",
    "        print(f\"Agent reached max_episode_steps in test.\")\n",
    "        break\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    \n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pre_screen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
