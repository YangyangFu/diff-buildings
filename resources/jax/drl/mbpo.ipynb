{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify, MBPO is an off-policy model-based reinforcement learning algorithm that combines model-free and model-based learning. It utilizes an ensemble of models to approximate the true environment dynamics, which helps with exploration and improves sample efficiency. The algorithm then uses these dynamics models to generate a buffer of imagined trajectories and updates the policy using a model-free method like Soft Actor-Critic (SAC) or Proximal Policy Optimization (PPO).\n",
    "\n",
    "In comparison, Dyna-Q is a model-based reinforcement learning algorithm that alternates between learning a model of the environment's dynamics and using that model to improve the policy through simulated experience. It does not leverage an ensemble of models, and it relies on Q-learning for the policy update step.\n",
    "\n",
    "Here are the main components of MBPO:\n",
    "\n",
    "- Model Learning: MBPO learns a probabilistic model of the environment's dynamics using collected data. This model is used to predict the next state and reward given the current state and action. It can be a neural network or any other function approximator.\n",
    "- Model Rollouts: Using the learned model, MBPO generates a set of trajectories by rolling out the model for multiple steps. These rollouts help in creating additional data points that can be used for training the policy and updating the value function.\n",
    "- Policy Optimization: The policy optimization is performed using model-free techniques, such as Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO), or Soft Actor-Critic (SAC). The objective is to optimize the policy using both real and model-generated data.\n",
    "- Iterative Update: MBPO alternates between model learning, model rollouts, and policy optimization, gradually improving the model's accuracy and the policy's performance.\n",
    "\n",
    "The main advantage of MBPO is that it reduces the number of interactions with the real environment by generating additional data using the learned model. This makes it more sample-efficient compared to model-free methods. However, the quality of the learned policy depends on the accuracy of the learned model, which can be sensitive to errors or biases in the model.\n",
    "\n",
    "Here's the high-level pseudocode for the MBPO implementation provided above:\n",
    "\n",
    "- Initialize environment, hyperparameters, experience buffer, dynamics model, policy model, and their respective optimizers.\n",
    "- Collect initial data from the environment and store it in the experience buffer.\n",
    "- For each epoch:\n",
    "  - Collect real data from the environment using the current policy and store it in the experience buffer.\n",
    "  - Train the dynamics model using data from the experience buffer.\n",
    "  - Generate model rollouts using the current dynamics model and policy. Store the generated data in the experience buffer.\n",
    "  - Optimize the policy using the combined data (real and model-generated) from the experience buffer.\n",
    "- Repeat for a specified number of epochs.\n",
    "\n",
    "The pseudocode represents the main components of the MBPO algorithm, which includes data collection, dynamics model training, model rollouts generation, and policy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "num_epochs = 1000\n",
    "rollout_steps = 5\n",
    "num_model_rollouts = 20\n",
    "num_model_updates = 5\n",
    "buffer_size = 10000\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    @nn.compact \n",
    "    def __call__(self, state, action) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        input = jnp.concatenate([state, action], axis=-1)\n",
    "        x = nn.Dense(64)(input)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        output = nn.Dense(state_dim + 1)(x)\n",
    "        return output\n",
    "\n",
    "class PolicyModel(nn.Module):\n",
    "    action_dim: jnp.int32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state) -> jnp.ndarray:\n",
    "        x = nn.Dense(64)(state)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(action_dim)(x)\n",
    "        return nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural networks and optimizers\n",
    "dynamics_model = DynamicsModel()\n",
    "policy_model = PolicyModel(2)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "dynamics_params = dynamics_model.init(rng_key, jnp.ones((1, state_dim)), jnp.ones((1, 1), dtype=jnp.int32))\n",
    "policy_params = policy_model.init(rng_key, jnp.ones((1, state_dim)))\n",
    "\n",
    "dynamics_optimizer = optax.adam(1e-3)\n",
    "policy_optimizer = optax.adam(1e-3)\n",
    "\n",
    "dynamics_opt_state = dynamics_optimizer.init(dynamics_params)\n",
    "policy_opt_state = policy_optimizer.init(policy_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def dynamics_loss(params, state, action, next_state, reward) -> jnp.ndarray:\n",
    "    predicted = DynamicsModel().apply(params, state, action)\n",
    "    target = jnp.concatenate([next_state, reward], axis=-1)\n",
    "    return jnp.mean(jnp.square(predicted - target))\n",
    "\n",
    "@jax.jit\n",
    "def policy_loss(params, state, action, advantage) -> jnp.ndarray:\n",
    "    logits = PolicyModel().apply(params, state)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    action_log_probs = jnp.take_along_axis(log_probs, action[..., None], axis=-1)\n",
    "    return -jnp.mean(action_log_probs * advantage)\n",
    "\n",
    "dynamics_grad_fn = jax.jit(jax.grad(dynamics_loss))\n",
    "policy_grad_fn = jax.jit(jax.grad(policy_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Experience Buffer class\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            self.buffer[self.position] = transition\n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in idx])\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 2. Initialize the experience buffer\n",
    "buffer = ExperienceBuffer(buffer_size)\n",
    "\n",
    "# 3. Collect initial data from the environment and add it to the experience buffer\n",
    "state = env.reset()\n",
    "for _ in range(buffer_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    buffer.add(state, action, reward, next_state, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "# 4. Perform model rollouts\n",
    "def rollout_model(params, state, policy_params, rollout_steps, rng_key):\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    for _ in range(rollout_steps):\n",
    "        action_probs = PolicyModel().apply(policy_params, state)\n",
    "        action = jax.random.categorical(rng_key, action_probs)\n",
    "        \n",
    "        next_state, reward = jnp.split(DynamicsModel().apply(params, state, action), [state_dim])\n",
    "        next_state, reward = jnp.squeeze(next_state), jnp.squeeze(reward)\n",
    "        done = False  # Assume the model rollouts don't reach terminal states\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training loop\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "for epoch in range(num_epochs):\n",
    "    # Collect real data\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = jax.random.choice(rng_key, action_dim, p=PolicyModel().apply(policy_params, state))\n",
    "        next_state, reward, done, _, _ = env.step(int(action))\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state if not done else env.reset()[0]\n",
    "\n",
    "    # Train the dynamics model\n",
    "    for _ in range(num_model_updates):\n",
    "        states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "        grads = dynamics_grad_fn(dynamics_params, states, actions, next_states, rewards)\n",
    "        updates, dynamics_opt_state = dynamics_optimizer.update(grads, dynamics_opt_state)\n",
    "        dynamics_params = optax.apply_updates(dynamics_params, updates)\n",
    "\n",
    "    # Generate model rollouts\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(num_model_rollouts):\n",
    "        rollout_rng_key, rng_key = jax.random.split(rng_key)\n",
    "        states, actions, rewards, next_states, dones = rollout_model(dynamics_params, state, policy_params, rollout_steps, rollout_rng_key)\n",
    "\n",
    "        for s, a, r, ns, d in zip(states, actions, rewards, next_states, dones):\n",
    "            buffer.add(s, a, r, ns, d)\n",
    "            \n",
    "        state, _ = env.reset()\n",
    "\n",
    "    # Optimize the policy using both real and model-generated data\n",
    "    for _ in range(100):\n",
    "        states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "        next_action_probs = PolicyModel().apply(policy_params, next_states)\n",
    "        next_action_values = jnp.sum(next_action_probs * DynamicsModel().apply(dynamics_params, next_states, jnp.arange(action_dim)[:, None]), axis=-1)\n",
    "        next_action_values = jnp.squeeze(next_action_values)\n",
    "        target_values = rewards + 0.99 * (1 - dones) * next_action_values\n",
    "        advantages = target_values - DynamicsModel().apply(dynamics_params, states, actions)\n",
    "        advantages = jnp.squeeze(advantages)\n",
    "\n",
    "        grads = policy_grad_fn(policy_params, states, actions, advantages)\n",
    "        updates, policy_opt_state = policy_optimizer.update(grads, policy_opt_state)\n",
    "        policy_params = optax.apply_updates(policy_params, updates)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
