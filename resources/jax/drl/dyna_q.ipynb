{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyna-Q\n",
    "\n",
    "- initialize Q(s,a) and Model(s,a)\n",
    "- Loop forever:\n",
    "  - S <- current nonterminal state\n",
    "  - A <- $\\epsilon$-greedy (S, Q)\n",
    "  - take action A; observe resultant reward, R and next state S'\n",
    "  - Q(S,A) <- Q(S,A) + $\\alpha$[R + $\\gamma$ $\\max_a$ Q(S', a) - Q(S, A)], direct RL \n",
    "  - Model(S,A) <- R, S', assuming deterministic environment, model update\n",
    "  - Loop repeat n times for planning:\n",
    "    - S <- random previously observed state\n",
    "    - A <- random action previously taken in S\n",
    "    - R, S' <- Model(S, A)\n",
    "    - Q(S,A) <- Q(S,A) + $\\alpha$[R + $\\gamma$ $\\max_a$ Q(S', a) - Q(S, A)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=2)(x)\n",
    "        return x\n",
    "\n",
    "class DynamicsModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=5)(x)\n",
    "        return x\n",
    "\n",
    "def epsilon_greedy_policy(q_values, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        return int(jnp.argmax(q_values))\n",
    "\n",
    "def dyna_q(env, q_params, q_opt_state, dynamics_params, dynamics_opt_state, episodes=500, planning_steps=5, epsilon=0.1, gamma=0.99, alpha=0.5):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "\n",
    "    # initialize q network\n",
    "    q_model = QNetwork()\n",
    "    q_values = lambda params, s: q_model.apply(params, s.reshape(1, -1))\n",
    "\n",
    "    # initialize dynamic network\n",
    "    dynamics_model = DynamicsModel()\n",
    "    predict_dynamics = lambda params, s, a: dynamics_model.apply(params, jnp.hstack([s, jnp.array([a])]).reshape(1, -1))\n",
    "\n",
    "    # initialize buffer\n",
    "    transition_memory = []\n",
    "\n",
    "    # for each episode\n",
    "    for episode in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # for each step\n",
    "        while not done:\n",
    "            # choose an action a from current state s based on eps-greedy strategy using q values\n",
    "            a = epsilon_greedy_policy(q_values(q_params, s), epsilon)\n",
    "            # execute the action and observe the resuting reward and the next state\n",
    "            s_next, r, done, _, _ = env.step(a)\n",
    "            # update buffer\n",
    "            transition_memory.append((s, a, r, s_next))\n",
    "\n",
    "            # calculate the target q-value for the current state-action pair\n",
    "            # [0] is batch\n",
    "            q_s = q_values(q_params, s)\n",
    "            target = q_s[0].copy()\n",
    "            if done:\n",
    "                target = target.at[a].set(r)\n",
    "            else:\n",
    "                target = target.at[a].set(r + gamma * jnp.max(q_values(q_params, s_next)))\n",
    "\n",
    "            # update q-network using temporal difference error, which is the difference between the current estimate of the q-value and the target-q value\n",
    "            loss_fn = lambda params: jnp.mean((q_model.apply(params, s.reshape(1, -1)) - target) ** 2)\n",
    "            losses, grad = jax.value_and_grad(loss_fn)(q_params)\n",
    "            #_, grad = grad_fn(q_params)\n",
    "            updates, q_opt_state = q_optimizer.update(grad, q_opt_state)\n",
    "            q_params = optax.apply_updates(q_params, updates)\n",
    "\n",
    "            # update the dynamic network if the episode is not done\n",
    "            if not done:\n",
    "                inputs = jnp.hstack([s, jnp.array([a])])\n",
    "                target_dynamics = jnp.hstack([r, s_next])\n",
    "                print(target_dynamics.shape)\n",
    "                print(dynamics_model.apply(dynamics_params, inputs.reshape(1, -1)))\n",
    "                loss_fn_dynamics = lambda params: jnp.mean((dynamics_model.apply(params, inputs.reshape(1, -1)) - target_dynamics) ** 2)\n",
    "                losses_dynamics, grad_dynamics = jax.value_and_grad(loss_fn_dynamics)(dynamics_params)\n",
    "                #_,grad_dynamics= grad_fn_dynamics(dynamics_params)\n",
    "                updates, dynamics_opt_state = dynamics_optimizer.update(grad_dynamics, dynamics_opt_state)\n",
    "                dynamics_params = optax.apply_updates(dynamics_params, updates)\n",
    "\n",
    "            # perform planning step using the transition memory\n",
    "            for _ in range(planning_steps):\n",
    "                # Sample a random transition from memory\n",
    "                s_sample, a_sample, r_sample, s_next_sample = random.choice(transition_memory)\n",
    "\n",
    "                # Use the environment model to predict the next state and reward\n",
    "                r_pred, s_next_pred = predict_dynamics(dynamics_params, s_sample, a_sample).ravel()[0], predict_dynamics(dynamics_params, s_sample, a_sample).ravel()[1:]\n",
    "\n",
    "                # Calculate the target Q-value for the sampled state-action pair\n",
    "                q_s_sample = q_values(q_params, s_sample)\n",
    "                target_sample = q_s_sample[0].copy()\n",
    "\n",
    "                if s_next_sample is None:\n",
    "                    target_sample = target_sample.at[a_sample].set(r_pred)\n",
    "                else:\n",
    "                    target_sample = target_sample.at[a_sample].set(r_pred + gamma * jnp.max(q_values(q_params, s_next_pred)))\n",
    "\n",
    "                # Update the Q-value function using the sampled transition\n",
    "                loss_fn_sample = lambda params: jnp.mean((q_model.apply(params, s_sample.reshape(1, -1)) - target_sample) ** 2)\n",
    "                _, grad_sample = jax.value_and_grad(loss_fn_sample)(q_params)\n",
    "                #grad_sample, _ = grad_fn_sample(q_params)\n",
    "                updates, q_opt_state = q_optimizer.update(grad_sample, q_opt_state)\n",
    "                q_params = optax.apply_updates(updates, q_params)\n",
    "\n",
    "\n",
    "            s = s_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "rng_q = jax.random.PRNGKey(0)\n",
    "rng_dynamics = jax.random.PRNGKey(1)\n",
    "\n",
    "q_model = QNetwork()\n",
    "q_params = q_model.init(rng_q, jnp.ones((1, 4)))\n",
    "q_optimizer = optax.adam(1e-4)\n",
    "q_opt_state = q_optimizer.init(q_params)\n",
    "\n",
    "dynamics_model = DynamicsModel()\n",
    "dynamics_params = dynamics_model.init(rng_dynamics, jnp.ones((1, 5)))\n",
    "dynamics_optimizer = optax.adam(1e-4)\n",
    "dynamics_opt_state = dynamics_optimizer.init(dynamics_params)\n",
    "\n",
    "dyna_q(env, q_params, q_opt_state, dynamics_params, dynamics_opt_state)\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
