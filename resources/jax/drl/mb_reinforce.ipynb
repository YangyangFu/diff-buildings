{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_network(rng, layer_sizes):\n",
    "    keys = random.split(rng, len(layer_sizes) - 1)\n",
    "    params = [(random.normal(rng, (in_dim, out_dim)) / np.sqrt(in_dim), np.zeros(out_dim))\n",
    "                for rng, in_dim, out_dim in zip(keys, layer_sizes[:-1], layer_sizes[1:])]\n",
    "    return params\n",
    "\n",
    "def forward(params, state):\n",
    "    activations = state\n",
    "    for W, b in params[:-1]:\n",
    "        activations = jnp.tanh(jnp.dot(activations, W) + b)\n",
    "    logits = jnp.dot(activations, params[-1][0]) + params[-1][1]\n",
    "    return logits\n",
    "\n",
    "def rewards_to_go(policy_params, model_params, state, action_dim, state_dim, discount_factor=0.99, n_steps=10):\n",
    "    rtg = 0\n",
    "    current_state = state\n",
    "    current_discount = 1.0\n",
    "    for _ in range(n_steps):\n",
    "        action_logits = forward(policy_params, current_state)\n",
    "        action_probs = jax.nn.softmax(action_logits)\n",
    "        action_probs = np.asarray(action_probs).astype(\"float64\")\n",
    "        action_probs_norm = action_probs/action_probs.sum()\n",
    "        # we might need implement a eps-greedy strategy\n",
    "        # currently use a stochastic strategy\n",
    "        action = np.random.choice(action_dim, p=action_probs_norm)\n",
    "        state_action = jnp.hstack([current_state, jnp.array([action])])\n",
    "        next_state_pred, reward_pred = jnp.split(forward(model_params, state_action), [state_dim])\n",
    "        rtg += current_discount * reward_pred\n",
    "        current_discount *= discount_factor\n",
    "        current_state = next_state_pred\n",
    "    return rtg\n",
    "\n",
    "@jit\n",
    "def loss(params, state, action, advantage):\n",
    "    logits = forward(params, state)\n",
    "    log_probs = logits - jax.scipy.special.logsumexp(logits)\n",
    "    log_prob = log_probs[action]\n",
    "    return jnp.sum(-log_prob * advantage)\n",
    "\n",
    "@jit\n",
    "def model_loss(params, state, action, target):\n",
    "    state_action = jnp.hstack([state, action])\n",
    "    prediction = forward(params, state_action)\n",
    "    return jnp.mean(jnp.square(target - prediction))\n",
    "\n",
    "policy_grad_loss = jit(grad(loss, argnums=0))\n",
    "model_grad_loss = jit(grad(model_loss, argnums=0))\n",
    "\n",
    "def train(rng, env, num_episodes=10000):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    policy_hidden_dim = 64\n",
    "    policy_params = create_network(rng, [state_dim, policy_hidden_dim, action_dim])\n",
    "\n",
    "    model_input_dim = state_dim + 1\n",
    "    model_hidden_dim = 64\n",
    "    model_output_dim = state_dim + 1\n",
    "    model_params = create_network(rng, [model_input_dim, model_hidden_dim, model_output_dim])\n",
    "\n",
    "    discount_factor = 0.99\n",
    "    lr = 1e-4\n",
    "    optimizer = optax.adam(lr)\n",
    "\n",
    "    opt_state_policy = optimizer.init(policy_params)\n",
    "    opt_state_model = optimizer.init(model_params)\n",
    "\n",
    "    rewards_history = []\n",
    "    \"\"\" This block cannot be jitted due to conditional argument update_model\n",
    "    @jit\n",
    "    def train_step(step, opt_state, params, state, action, target, update_model=True):\n",
    "        if update_model:\n",
    "            g = model_grad_loss(params, state, action, target)\n",
    "        else:\n",
    "            g = policy_grad_loss(params, state, action, target)\n",
    "        updates, new_opt_state = optimizer.update(g, opt_state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_opt_state, new_params\n",
    "    \"\"\"\n",
    "    @jit\n",
    "    def train_policy_step(step, opt_state, params, state, action, advantage):\n",
    "        g = policy_grad_loss(params, state, action, advantage)\n",
    "        updates, new_opt_state = optimizer.update(g, opt_state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_opt_state, new_params\n",
    "\n",
    "    @jit\n",
    "    def train_model_step(step, opt_state, params, state, action, target):\n",
    "        g = model_grad_loss(params, state, action, target)\n",
    "        updates, new_opt_state = optimizer.update(g, opt_state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_opt_state, new_params\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action_logits = forward(policy_params, state)\n",
    "            action_probs = jax.nn.softmax(action_logits)\n",
    "            action_probs = np.asarray(action_probs).astype(\"float64\")\n",
    "            action_probs_norm = action_probs/action_probs.sum()\n",
    "            action = np.random.choice(action_dim, p=action_probs_norm)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            rtg = rewards_to_go(policy_params, model_params, state, action_dim, state_dim, discount_factor)\n",
    "            advantage = jnp.array(reward + discount_factor * rtg - rewards_to_go(policy_params, model_params, state, action_dim, state_dim))\n",
    "\n",
    "            opt_state_model, model_params = train_model_step(0, opt_state_model, model_params, state, jnp.array([action]), jnp.hstack([next_state, jnp.array([reward])]))\n",
    "            opt_state_policy, policy_params = train_policy_step(0, opt_state_policy, policy_params, state, jnp.array([action]), advantage)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "    return policy_params, rewards_history\n",
    "\n",
    "# Training and plotting the results\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "rng = random.PRNGKey(0)\n",
    "policy_params, rewards_history = train(rng, env)\n",
    "\n",
    "plt.plot(rewards_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
