{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double deep q learning\n",
    "\n",
    "The difference between DDQN and DQN:\n",
    "- In DQN, the same network is used to select the best action and to estimate the value of that action. This can lead to an overoptimistic estimation of Q-values, which may result in suboptimal policies.\n",
    "- DDQN decouples the action selection and action value estimation by using two separate networks: the online Q-network (with weights θ) and the target Q-network (with weights θ').\n",
    "  - the online Q-network is used to select the best action, the target Q-network is used to estimate the value of that action\n",
    "  - the target value computation in DDQN is as follows:\n",
    "    - use the online Q to select the best action\n",
    "    - use the target Q to estimate the value of taking this action\n",
    "    - compute the target value using Bellman optimality with the target Q-network\n",
    "  - the online Q network is updated as DQN, and the target Q-network is updated periodically by copying the weights from the online Q-network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    action_size: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(features=64)\n",
    "        self.dense2 = nn.Dense(features=64)\n",
    "        self.dense3 = nn.Dense(features=self.action_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(self.dense1(x))\n",
    "        x = nn.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size, rng_key, buffer_size=10000, batch_size=64, gamma=0.99, lr=1e-3, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, update_target_every=1000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_target_every = update_target_every\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "\n",
    "        rng_key, rng_key_init = jax.random.split(rng_key)\n",
    "        self.network = QNetwork(action_size)\n",
    "        self.params = self.network.init(rng_key_init, jnp.ones((state_size,)))\n",
    "        self.target_params = self.params\n",
    "        self.optimizer = optax.adam(self.lr)\n",
    "        self.opt_state = self.optimizer.init(self.params)\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "    def sync_target(self):\n",
    "        self.target_params = self.params\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = jnp.expand_dims(jnp.array(state, dtype=jnp.float32), axis=0)\n",
    "            q_values = self.network.apply(self.params, state)\n",
    "            return int(jnp.argmax(q_values))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        states = jnp.array(states, dtype=jnp.float32)\n",
    "        actions = jnp.array(actions, dtype=jnp.int32)\n",
    "        rewards = jnp.array(rewards, dtype=jnp.float32)\n",
    "        next_states = jnp.array(next_states, dtype=jnp.float32)\n",
    "        dones = jnp.array(dones, dtype=jnp.float32)\n",
    "\n",
    "        def loss(params):\n",
    "            q_values = self.network.apply(params, states)\n",
    "            online_q_next = self.network.apply(params, next_states)\n",
    "            target_q_next = self.network.apply(self.target_params, next_states)\n",
    "            next_action = jnp.argmax(online_q_next, axis=-1)\n",
    "            q_target_next = jax.vmap(lambda s: s[next_action])(target_q_next)\n",
    "            targets = rewards + self.gamma * (1 - dones) * q_target_next\n",
    "\n",
    "            q_values = jax.vmap(lambda s: s[actions])(q_values)\n",
    "            return jnp.mean((targets - q_values) ** 2)\n",
    "\n",
    "        grad_fn = jax.value_and_grad(loss)\n",
    "        loss_value, gradients = grad_fn(self.params)\n",
    "        updates, self.opt_state = self.optimizer.update(gradients, self.opt_state)\n",
    "        self.params = optax.apply_updates(self.params, updates)\n",
    "\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_target_every == 0:\n",
    "            self.sync_target()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing the DDQN agent\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "agent = DDQNAgent(state_size, action_size, rng_key)\n",
    "\n",
    "n_episodes = 500\n",
    "reward_history = []\n",
    "max_episode_steps=200 # env.spec.max_episode_steps\n",
    "reward_threshold=175 # env.spec.reward_threshold\n",
    "solved_window = 100\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_in_episode = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_in_episode += 1\n",
    "\n",
    "        # check if the max_episode_steps are met. if so, terminate this episode\n",
    "        if step_in_episode >= max_episode_steps:\n",
    "            print(f\"Agent reached max_episode_steps in episode {episode}.\")\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # stop training if average reward reaches requirement\n",
    "    # Calculate the average reward over the last 'solved_window' episodes\n",
    "    if episode >= solved_window:\n",
    "        avg_reward = np.mean(reward_history[-solved_window:])\n",
    "        print(f'Episode: {episode}, Average Reward: {avg_reward}')\n",
    "\n",
    "        if avg_reward >= reward_threshold:\n",
    "            print(f\"CartPole-v1 solved in {episode} episodes!\")\n",
    "            break\n",
    "\n",
    "# Plot the historical rewards\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Historical Rewards for CartPole-v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training \n",
    "def plot_moving_average_reward(episode_rewards, window_size=10):\n",
    "    cumsum_rewards = np.cumsum(episode_rewards)\n",
    "    moving_avg_rewards = (cumsum_rewards[window_size:] - cumsum_rewards[:-window_size]) / window_size\n",
    "\n",
    "    plt.plot(moving_avg_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title('Moving Average Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "plot_moving_average_reward(reward_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# need a virtual display for rendering in docker\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Test the trained agent\n",
    "n_test_episodes = 10\n",
    "\n",
    "print(\"\\nTesting the trained agent...\")\n",
    "for episode in range(n_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    pre_screen = env.render()\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "        screen = env.render()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        plt.imshow(screen)\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
