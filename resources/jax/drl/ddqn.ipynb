{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double deep q learning\n",
    "\n",
    "The difference between DDQN and DQN:\n",
    "- In DQN, the same network is used to select the best action and to estimate the value of that action. This can lead to an overoptimistic estimation of Q-values, which may result in suboptimal policies.\n",
    "- DDQN decouples the action selection and action value estimation by using two separate networks: the online Q-network (with weights θ) and the target Q-network (with weights θ').\n",
    "  - the online Q-network is used to select the best action, the target Q-network is used to estimate the value of that action\n",
    "  - the target value computation in DDQN is as follows:\n",
    "    - use the online Q to select the best action\n",
    "    - use the target Q to estimate the value of taking this action\n",
    "    - compute the target value using Bellman optimality with the target Q-network\n",
    "  - the online Q network is updated as DQN, and the target Q-network is updated periodically by copying the weights from the online Q-network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax import optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def setup(self):\n",
    "        self.hidden = nn.Dense(64)\n",
    "        self.output = nn.Dense(2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(self.hidden(x))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64,\n",
    "                 update_target_every=200, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_every = update_target_every\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.steps = 0\n",
    "\n",
    "        self.Q = QNetwork()\n",
    "        self.target_Q = QNetwork()\n",
    "        self.sync_target()\n",
    "\n",
    "        self.optimizer = optim.Adam(learning_rate=learning_rate).create(self.Q)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sync_target(self):\n",
    "        self.target_Q = self.Q\n",
    "\n",
    "    def act(self, state, apply_epsilon=True):\n",
    "        if apply_epsilon and np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        return jnp.argmax(self.Q(state))\n",
    "\n",
    "    @jax.jit\n",
    "    def forward_pass(self, params, state):\n",
    "        return self.Q.apply(params, state)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = jnp.array(states)\n",
    "        actions = jnp.array(actions)\n",
    "        rewards = jnp.array(rewards)\n",
    "        next_states = jnp.array(next_states)\n",
    "        dones = jnp.array(dones)\n",
    "\n",
    "        def loss_fn(params):\n",
    "            # q values for individual action q(s,a)\n",
    "            q_values = self.forward_pass(params, states)\n",
    "            next_q_values = self.forward_pass(params, next_states)\n",
    "            target_q_values = self.forward_pass(self.target_Q.params, next_states)\n",
    "\n",
    "            # q value for taking the action generated from the agent\n",
    "            action_indices = jax.ops.index_update(jnp.zeros_like(q_values), jax.ops.index[np.arange(self.batch_size), actions], 1)\n",
    "            q_values = jnp.sum(q_values * action_indices, axis=1)\n",
    "            # target q value for taking the best action based on target_1_values\n",
    "            best_actions = jnp.argmax(next_q_values, axis=1)\n",
    "            action_indices = jax.ops.index_update(jnp.zeros_like(target_q_values), jax.ops.index[np.arange(self.batch_size), best_actions], 1)\n",
    "            target_q_values = jnp.sum(target_q_values * action_indices, axis=1)\n",
    "\n",
    "            targets = rewards + (1 - dones) * self.gamma * target_q_values\n",
    "            return jnp.mean(jnp.square(q_values - targets))\n",
    "\n",
    "        grad_fn = jax.value_and_grad(loss_fn)\n",
    "        loss, grad = grad_fn(self.Q.params)\n",
    "        self.optimizer = self.optimizer.apply_gradient(grad)\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "    # train DDQN with given episodes.\n",
    "    # The training should stop if the average reward over the most recent 100 episodes is greater than 195\n",
    "    def train(self, env, episodes, render=False, solved_window=100, max_episode_steps=200, reward_threshold=175):\n",
    "        episode_rewards = []\n",
    "\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            state = jnp.array(state, dtype=jnp.float32)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps_in_episode = 0\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                self.steps += 1\n",
    "                steps_in_episode += 1\n",
    "\n",
    "                if self.steps % self.update_target_every == 0:\n",
    "                    self.sync_target()\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "                # Check if the agent reached max_episode_steps in a single episode\n",
    "                if steps_in_episode >= max_episode_steps:\n",
    "                    print(f\"Agent reached max_episode_steps in episode {e}\")\n",
    "                    break\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "            # Calculate the average reward over the last 'solved_window' episodes\n",
    "            if e >= solved_window:\n",
    "                avg_reward = np.mean(episode_rewards[-solved_window:])\n",
    "                print(f'Episode: {e}, Average Reward: {avg_reward}')\n",
    "\n",
    "                if avg_reward >= reward_threshold:\n",
    "                    print(f\"CartPole-v1 solved in {e} episodes!\")\n",
    "                    break\n",
    "\n",
    "        return episode_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 500\n",
    "episode_rewards = agent.train(env, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training \n",
    "def plot_moving_average_reward(episode_rewards, window_size=10):\n",
    "    cumsum_rewards = np.cumsum(episode_rewards)\n",
    "    moving_avg_rewards = (cumsum_rewards[window_size:] - cumsum_rewards[:-window_size]) / window_size\n",
    "\n",
    "    plt.plot(moving_avg_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title('Moving Average Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "plot_moving_average_reward(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# need a virtual display for rendering in docker\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Test the trained agent\n",
    "n_test_episodes = 10\n",
    "\n",
    "print(\"\\nTesting the trained agent...\")\n",
    "for episode in range(n_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = jnp.array(state, dtype=jnp.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    pre_screen = env.render()\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = jnp.array(next_state, dtype=jnp.float32)\n",
    "        screen = env.render()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        plt.imshow(screen)\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
