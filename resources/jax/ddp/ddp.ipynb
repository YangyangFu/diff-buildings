{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Dynamic Programming\n",
    "This section is to solve the Bellman equation using differentiable dynamic programming (DDP).\n",
    "Refer to [Differentiable Optimal Control via Differential Dynamic Programming](https://arxiv.org/pdf/2209.01117.pdf) and a Python implementation [here](https://github.com/neka-nat/ddp-gym/blob/master/ddp_gym.py).\n",
    "\n",
    "DDP is designed for solving nonlinear dynamic system optimal control, which performs a quadratic approximation of the cost based on Taylor Series.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Dynamics\n",
    "\n",
    "\\begin{align}\n",
    "x_{t+1} &= f(x_t, u_t) \\\\\n",
    "x_0 &= x(0)\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The `cost-to-go` function at time `t` is shown as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    J(x_t, U_t) &= \\sum_{k=t}^{N-1}\\ell(x_k, u_k) + \\ell_f(x_N) \\\\\n",
    "    U_t &= \\{u_t, ..., u_{N-1}\\}\n",
    "\\end{align}\n",
    "\n",
    "Minimum `cost-to-go` or `value function` at time `t` is then:\n",
    "\n",
    "\\begin{align}\n",
    "    V_t(x_t) &= \\min_{U_t} J_t(x_t, U_t) \\\\\n",
    "            &= \\min_{u_t, ..., u_{N-1}} \\sum_{k=t}^{N-1} \\ell(x_k, u_k) + \\ell_f(x_{N}) \\\\\n",
    "            &= \\min_{u_t} [\\ell(x_t, u_t) + V_{t+1}(x_{t+1})] \\\\\n",
    "            &= \\min_{u_t} [\\ell(x_t, u_t) + V_{t+1}(f(x_t, u_t)] \\\\\n",
    "            &= \\min_{u_t} Q_t(x_t, u_t)\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order Approximation of Q-function\n",
    "For linear system and quadratic cost function $\\ell$, $Q$ is quadratic.\n",
    "But when the system is nonlinear, $Q$ is no longer quadratic.\n",
    "DDP is to approximate the action-value function $Q$ in a quadratic format, which can be done based on Taylor Series.\n",
    "\n",
    "DDP iterates between\n",
    "- (i) minimizing the quadratic approximation of the Q-function in a `backward pass`, and \n",
    "- (ii) integrating the system dynamics in a `forward pass`\n",
    "\n",
    "Given a sequence of nominal control trajectory $U_t^r = \\{u_t^r, ..., u_{N-1}^r\\}$, and a sequence of nominal state trajectory $X_t^r = \\{x_t^r, ..., x_N^r\\}$, we can approximate $Q_t(u_t, x_t)$ around the given nominal conditions based on Taylor series. \n",
    "Let's drop the subscript `t` for brivity:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat Q(x_t,u_t) = Q(x_t^r, u_t^r) + Q_x \\Delta x_t + Q_u \\Delta u_t + \\frac{1}{2}\\Delta x_t^T Q_{xx} \\Delta x_t + \\frac{1}{2}\\Delta u_t^T Q_{uu} \\Delta u_t + \\Delta u_t^TQ_{ux} \\Delta x_t \n",
    "\\end{equation}\n",
    "\n",
    "where $\\Delta u_t = u_t - u_t^r$, and $\\Delta x_t = x_t - x_t^r$.\n",
    "\n",
    "The first-order and second-order partial derivatives of $Q$ can be calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "Q_x &= \\ell_x + f_x^TV_{x,t+1} \\\\\n",
    "Q_u &= \\ell_u + f_u^TV_{x,t+1} \\\\\n",
    "Q_{xx} &= \\ell_{xx} + f_x^TV_{xx,t+1} f_x + V_{x,t+1} f_{xx} \\\\\n",
    "Q_{uu} &= \\ell_{uu} + f_u^TV_{uu,t+1} f_u + V_{x,t+1} f_{uu} \\\\\n",
    "Q_{ux} &= \\ell_{ux} + f_u^TV_{ux,t+1} f_x + V_{x,t+1} f_{ux}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Then optimal control problem then becomes\n",
    "\\begin{equation}\n",
    "    \\Delta u^* = \\argmin_{\\Delta u} \\hat Q(x, u) = -Q_{uu}^{-1}Q_u - Q_{uu}^{-1}Q_{ux}\\Delta x = -k - K\\Delta x\n",
    "\\end{equation}\n",
    "\n",
    "Use this optimal $\\Delta u$ and known nominal control inputs $u^r$, we can then calculate the optimal cost-to-go $V$ in a backward pass from $t = N$ to $t = 1$:\n",
    "\n",
    "\\begin{align}\n",
    "u_t^* &= u_t^r + \\Delta u_t^* \\\\\n",
    "V(x_t) &= Q(x_t, u_t^*) = Q(x_t^r, u_t^r) - Q_uQ_{uu}^{-1}Q_u \\\\\n",
    "V_x &= Q_x - Q_{ux}Q_{uu}^{-1}Q_u \\\\ \n",
    "V_{xx} &= Q_{xx} - Q_{ux}Q_{uu}^{-1}Q_{ux}\n",
    "\\end{align}\n",
    "\n",
    "Then we can perform a forward pass from $t= 0$ to $t=N-1$ to calculate the new nominal state trajectories using the optimal control inputs:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat x_0 &= x(0) \\\\\n",
    "\\Delta \\hat u_t &= \\hat u_t - u_t^r = -k_t - K_t(\\hat x_t - x_t^r) \\\\\n",
    "\\hat x_{t+1} &= f(\\hat x_t, \\hat u_t)\n",
    "\\end{align}\n",
    "\n",
    "The backward pass and forward pass are iterated until convergence.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure**\n",
    "- generate a nominal trajectory for control inputs $\\{u_0^r, u_1^r, ..., u_{N-1}^r\\}$\n",
    "- forward pass to calculate the nominal trajectory for system states $\\{x_0^r, x_1^r, ..., x_N^r\\}$\n",
    "- set $V_N(x_N) = \\ell_f(x_N)$, and calculate $V_x$ and $V_{xx}$ at $t = N$\n",
    "- backward pass: for t = N, ..., 1:\n",
    "  - calculate $Q_x$, $Q_u$, $Q_{xx}$, $Q_{uu}$ and $Q_{ux}$ at $t-1$\n",
    "  - calculate $k_{t-1}, K_{t-1}$\n",
    "- initiate $\\hat x_0 = x(0)$\n",
    "- forward pass: for t = 0, ..., N-1:\n",
    "  - calculate $\\Delta x_t = \\hat x_t - x_t^r$\n",
    "  - calculate control input changes $\\Delta \\hat u_t$ based on $k_t$, $K_t$ and $\\Delta x_t$\n",
    "  - calculate improved control input $\\hat u_t = u_t^r + \\Delta \\hat u_t$\n",
    "  - calculate improved state trajectory $\\hat x_{t+1}$\n",
    "- repeat until convergence\n",
    "  - $V(x_0)$ is minimized and converged\n",
    "  - max iteration is reached \n",
    "  - etc ... \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deveopment Ideas\n",
    "- DDP class\n",
    "- interact with openGym environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Jan 26 2023, 10:47:49) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
